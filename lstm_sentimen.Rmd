---
---
---

## Load Library

```{r}
library(keras)
library(tensorflow)
# install.packages("magrittr")
library(magrittr)
# install.packages("tidyverse")
library(tidyverse)
# install.packages("textclean")
library("textclean")
# install.packages("tm") # for text mining
library("tm")
# install.packages("SnowballC") # for text stemming
library("SnowballC")
# install.packages("wordcloud") # word-cloud generator 
library("wordcloud")
# install.packages("syuzhet") # for sentiment analysis
library("syuzhet")
# install.packages("RColorBrewer") # color palettes
# install.packages("ggplot2") # for plotting graphs
library("RColorBrewer")
library("ggplot2")

```

```{r}
# install.packages("magrittr")
library(magrittr)
# install.packages("tidyverse")
library(tidyverse)
# install.packages("textclean")
library("textclean")
# install.packages("tm") # for text mining
library("tm")
# install.packages("SnowballC") # for text stemming
library("SnowballC")
# install.packages("wordcloud") # word-cloud generator 
library("wordcloud")
# install.packages("RColorBrewer") # color palettes
# install.packages("ggplot2") # for plotting graphs
library("RColorBrewer")
library("ggplot2")
```

## Getting Dataset

```{r}
imdb <- dataset_imdb(num_words = 500)
```

## Combine dataset and Checking vector/list's length

```{r}
c(c(train_x, train_y), c(test_x, test_y)) %<-% imdb
length(train_x); length(test_x)
```

## Frequency results from tabulation of categorical variables

```{r}

#Train
table(train_y)

print("         ")


#Test
table(test_y)
```

## Extract unique integers that represent the words in the movie review.

```{r}
train_x[[10]]
```

## Padding Sequences

```{r}
train_x <- pad_sequences(train_x, maxlen = 90)

test_x <- pad_sequences(test_x, maxlen = 90)
```

```{r}
train_x[10,]
```

## Model

### LSTM

```{r}
modelLSTM <- keras_model_sequential()
modelLSTM %>%
  layer_embedding(input_dim = 500, output_dim = 32) %>%
  layer_lstm(units = 32,return_sequences = TRUE) %>% 
  layer_lstm(units = 32,return_sequences = TRUE) %>%
  layer_lstm(units = 32) %>%
  layer_dense(units = 1, activation = "sigmoid")
```

## Compile Model

### LSTM

```{r}
#model %>% compile(optimizer = "rmsprop",
modelLSTM %>% compile(optimizer = "adam",
                  loss = "binary_crossentropy",
                  metrics = c("acc"))
```

## Fit Model

### LSTM

```{r}
historyLSTM <- modelLSTM %>% fit(train_x, train_y,
                         epochs = 25,
                         batch_size = 128,
                         validation_split = 0.2)
plot(historyLSTM)
```

## Model Prediction

### Train

```{r}
modelLSTM %>% evaluate(train_x, train_y)
```

```{r}
#pred <- model %>% predict_classes(train_x)
#pred <- model %>% predict(train_x)
#classes <- which.max(pred)

pred <- (modelLSTM %>% predict(train_x) > 0.5)

table(Predicted=pred, Actual=imdb$train$y)
```

### Test

```{r}
modelLSTM %>% evaluate(test_x, test_y)
```

```{r}
pred1 <- (modelLSTM %>% predict(test_x) > 0.5)

table(Predicted=pred1, Actual=imdb$test$y)
```

## Read file data

```{r}
#text <- read_csv(file.choose())
data <- read_csv("clean_review.csv") %>%
  select(reviewText) %>% 
  na.omit() # remove NA

head(data,5)
```

```{r}
data <- replace_non_ascii(data, replacement = "", remove.nonconverted = TRUE)
```

```{r}
data
```

```{r}
# Encoding(data) <- "latin1" # soalnya teks semua
```

```{r}
# Load the data as a corpus
TextDoc <- Corpus(VectorSource(data))

# corpus in r : collection of text document(s) to apply text mining or NLP routines on
```

```{r}
TextDoc
```

```{r}
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")

# Convert the text to lower case
TextDoc <- tm_map(TextDoc, content_transformer(tolower))

# Remove numbers
TextDoc <- tm_map(TextDoc, removeNumbers)

# Remove english common stopwords
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))

## Remove your own stop word
## specify your custom stopwords as a character vector
#TextDoc <- tm_map(TextDoc, removeWords, c("s", "company", "team")) 

# Remove punctuations
TextDoc <- tm_map(TextDoc, removePunctuation)

# Eliminate extra white spaces
TextDoc <- tm_map(TextDoc, stripWhitespace)

## Text stemming - which reduces words to their root form
#TextDoc <- tm_map(TextDoc, stemDocument)
```

```{r}
# Build a term-document matrix to count the occurrence of each word, to identify popular or trending topics
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)

# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 10 most frequent words

head(dtm_d, 10)
```

```{r}
dtm_v
```

```{r}
# Plot the most frequent words with bar chart
barplot(dtm_d[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col ="lightgreen", main ="Top 5 most frequent words",
        ylab = "Word frequencies")
```

```{r}
# Plot the most frequent words with bar chart
barplot(dtm_d[1:10,]$freq, las = 2, names.arg = dtm_d[1:10,]$word,
        col ="lightblue", main ="Top 10 most frequent words",
        ylab = "Word frequencies")
```

```{r}
# Plot the most frequent words with bar chart
barplot(dtm_d[1:25,]$freq, las = 2, names.arg = dtm_d[1:25,]$word,
        col ="lightblue", main ="Top 25 most frequent words",
        ylab = "Word frequencies")
```

```{r}
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))
```

```{r}
# Find associations corlimit only ver
findAssocs(TextDoc_dtm, terms = c("room","hotel","good"), corlimit = 0.25)
```

```{r}
# Find associations for words that occur at least 50 times
findAssocs(TextDoc_dtm, terms = findFreqTerms(TextDoc_dtm, lowfreq = 50), corlimit = 0.25)
```

## **Emotion Classification w NRC Word-Emotion Association Lexicon (aka EmoLex)**

```{r}
data_c <- TextDoc$content
data_c
```

```{r}
# run nrc sentiment analysis to return data frame with each row classified as one of the following
# emotions, rather than a score: 
# anger, anticipation, disgust, fear, joy, sadness, surprise, trust 
# It also counts the number of positive and negative emotions found in each row
d<-get_nrc_sentiment(data_c)
```

```{r}
# head(d,10) - to see top 10 lines of the get_nrc_sentiment dataframe
head (d,10)
```

```{r}
#transpose
td<-data.frame(t(d))
#The function rowSums computes column sums across rows for each level of a grouping variable.
td_new <- data.frame(rowSums(td))
#Transformation and cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(sentiment, data=td_new2, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Hotel Review Sentiments")
```

```{r}
#transpose
td<-data.frame(t(d))
#The function rowSums computes column sums across rows for each level of a grouping variable.
td_new <- data.frame(rowSums(td))
#Transformation and cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:10,]
#Plot One - count of words associated with each sentiment
quickplot(sentiment, data=td_new2, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Hotel Review Sentiments")
```

```{r}
#transpose
td<-data.frame(t(d))
#The function rowSums computes column sums across rows for each level of a grouping variable.
td_new <- data.frame(rowSums(td))
#Transformation and cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[9:10,]
#Plot One - count of words associated with each sentiment
quickplot(sentiment, data=td_new2, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Hotel Review Sentiments")
```

```{r}
#Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
  sort(colSums(prop.table(d[, 1:8]))), 
  horiz = TRUE, 
  col = "lightgreen",
  cex.names = 0.7, 
  las = 1, 
  main = "8 Emotions Percentage in Text", xlab="Percentage"
)
```

```{r}
barplot(
  sort(colSums(prop.table(d[, 9:10]))), 
  horiz = TRUE, 
  col = "lightyellow",
  cex.names = 0.7, 
  las = 1, 
  main = "Sentiment Percentage in Text", xlab="Percentage"
)
```

Next : coba embedding dataset training yang juga bisa diterapkan di data yang mau di-analisis
