---
---
---

## Load Library

```{r}
library(keras)
library(tensorflow)
#install.packages("magrittr")
library(magrittr)
```

## Getting Dataset

```{r}
imdb <- dataset_imdb(num_words = 500)
```

## Combine dataset and Checking vector/list's length

```{r}
c(c(train_x, train_y), c(test_x, test_y)) %<-% imdb
length(train_x); length(test_x)
```

## Frequency results from tabulation of categorical variables

```{r}

#Train
table(train_y)

print("         ")


#Test
table(test_y)
```

## Extract unique integers that represent the words in the movie review.

```{r}
train_x[[10]]
```

## Padding Sequences

```{r}
train_x <- pad_sequences(train_x, maxlen = 90)

test_x <- pad_sequences(test_x, maxlen = 90)
```

```{r}
train_x[10,]
```

## Model

### BiLSTM

```{r}
modelBiLSTM <- keras_model_sequential()
modelBiLSTM %>%
  layer_embedding(input_dim = 500, output_dim = 32) %>%
  layer_lstm(units = 32,return_sequences = TRUE) %>%
  layer_lstm(units = 32,return_sequences = TRUE) %>%
  bidirectional(layer_lstm(units = 32)) %>%
  layer_dense(units = 1, activation = "sigmoid")
```

## Compile Model

### BiLSTM

```{r}
#model %>% compile(optimizer = "rmsprop",
modelBiLSTM %>% compile(optimizer = "adam",
                  loss = "binary_crossentropy",
                  metrics = c("acc"))
```

## Fit Model

### BiLSTM

```{r}
historyBiLSTM <- modelBiLSTM %>% fit(train_x, train_y,
                         epochs = 25,
                         batch_size = 128,
                         validation_split = 0.2)
plot(historyBiLSTM)
```

## Model Prediction

### Train

```{r}
modelBiLSTM %>% evaluate(train_x, train_y)
```

```{r}
#pred <- model %>% predict_classes(train_x)
#pred <- model %>% predict(train_x)
#classes <- which.max(pred)

pred <- (modelBiLSTM %>% predict(train_x) > 0.5)

table(Predicted=pred, Actual=imdb$train$y)
```

### Test

```{r}
modelBiLSTM %>% evaluate(test_x, test_y)
```

```{r}
pred1 <- (modelBiLSTM %>% predict(test_x) > 0.5)

table(Predicted=pred1, Actual=imdb$test$y)
```

### Read file data

```{r}
text <- readLines(file.choose())

head(text,5)
```

```{r}
text <- replace_non_ascii(text, replacement = "", remove.nonconverted = TRUE)
```

```{r}
Encoding(text) <- "latin1" # soalnya teks semua
```

```{r}
# Load the data as a corpus
TextDoc <- Corpus(VectorSource(text))

# corpus in r : collection of text document(s) to apply text mining or NLP routines on
```

```{r}
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
# Convert the text to lower case
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
# Remove numbers
TextDoc <- tm_map(TextDoc, removeNumbers)
# Remove english common stopwords
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
# Remove your own stop word
# specify your custom stopwords as a character vector
TextDoc <- tm_map(TextDoc, removeWords, c("s", "company", "team")) 
# Remove punctuations
TextDoc <- tm_map(TextDoc, removePunctuation)
# Eliminate extra white spaces
TextDoc <- tm_map(TextDoc, stripWhitespace)
# Text stemming - which reduces words to their root form
TextDoc <- tm_map(TextDoc, stemDocument)
```

```{r}
# Build a term-document matrix to count the occurrence of each word, to identify popular or trending topics
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 10 most frequent words

head(dtm_d, 10)
```

```{r}
# Plot the most frequent words with bar chart
barplot(dtm_d[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col ="lightgreen", main ="Top 5 most frequent words",
        ylab = "Word frequencies")
```

```{r}
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))
```

```{r}
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))
```

```{r}
# Find associations corlimit only ver
findAssocs(TextDoc_dtm, terms = c("good","work","health"), corlimit = 0.25)
```

```{r}
# Find associations for words that occur at least 50 times
findAssocs(TextDoc_dtm, terms = findFreqTerms(TextDoc_dtm, lowfreq = 50), corlimit = 0.25)
```

## Sentiment Scores w/ Syuzhet package

```{r}
# regular sentiment score using get_sentiment() function and method of your choice
# please note that different methods may have different scales
syuzhet_vector <- get_sentiment(text, method="syuzhet")
# see the first row of the vector
head(syuzhet_vector)
```
